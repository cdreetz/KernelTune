#!/usr/bin/env python3
"""
Script to upload the SFT dataset to Hugging Face Hub
"""

import os
import json
from datasets import Dataset
from huggingface_hub import HfApi, login
import argparse

def load_jsonl_dataset(file_path):
    """
    Load dataset from JSONL file generated by generator.py
    
    Args:
        file_path: Path to the JSONL file
        
    Returns:
        List of examples
    """
    print(f"Loading dataset from {file_path}...")
    
    dataset = []
    with open(file_path, 'r') as f:
        for line in f:
            dataset.append(json.loads(line.strip()))
    
    print(f"✓ Loaded {len(dataset)} examples")
    return dataset

def convert_to_hf_format(dataset):
    """
    Convert generator.py format to standardized format
    
    Args:
        dataset: List of examples from generator.py
        
    Returns:
        Dataset with standardized format
    """
    print("Converting to standardized format...")
    
    formatted_data = []
    for example in dataset:
        formatted_example = {
            "prompt": example["instruction"],
            "completion": example["response"],
            "id": example["id"],
            "operation": example.get("operation", ""),
            "type": example["type"],
            "has_triton_docs": example["has_triton_docs"],
            "pytorch_code": example.get("pytorch_code", "")
        }
        formatted_data.append(formatted_example)
    
    hf_dataset = Dataset.from_list(formatted_data)
    print(f"✓ Converted {len(hf_dataset)} examples")
    
    return hf_dataset

def upload_dataset_to_hf(
    dataset_path,
    repo_name="triton-sft-dataset",
    username=None,
    private=False
):
    """
    Upload the SFT dataset to Hugging Face Hub
    
    Args:
        dataset_path: Path to the JSONL dataset file
        repo_name: Name for the repository on HF Hub
        username: Your HF username (if None, will be inferred from token)
        private: Whether to make the dataset private
    """
    
    # Login to Hugging Face
    print("Logging in to Hugging Face...")
    try:
        login()
        print("✓ Successfully logged in to Hugging Face")
    except Exception as e:
        print(f"✗ Failed to login: {e}")
        print("Please run: huggingface-cli login")
        return False
    
    # Load and convert the dataset
    try:
        dataset_list = load_jsonl_dataset(dataset_path)
        dataset = convert_to_hf_format(dataset_list)
    except Exception as e:
        print(f"✗ Failed to load/convert dataset: {e}")
        return False
    
    # Get user info if username not provided
    api = HfApi()
    if username is None:
        try:
            user_info = api.whoami()
            username = user_info["name"]
            print(f"✓ Using username: {username}")
        except Exception as e:
            print(f"✗ Failed to get user info: {e}")
            return False
    
    # Create repository name
    repo_id = f"{username}/{repo_name}"
    
    # Count examples by type and docs usage
    total_examples = len(dataset)
    synthetic_count = sum(1 for ex in dataset if ex['type'] == 'synthetic')
    convert_count = sum(1 for ex in dataset if ex['type'] == 'convert')
    with_docs_count = sum(1 for ex in dataset if ex['has_triton_docs'])
    without_docs_count = total_examples - with_docs_count
    
    # Add dataset card content
    dataset_card = f"""---
license: apache-2.0
task_categories:
- text-generation
language:
- en
tags:
- triton
- gpu-kernels
- code-generation
- synthetic-data
size_categories:
- 1K<n<10K
---

# Triton Kernel SFT Dataset

This dataset contains {total_examples} examples for supervised fine-tuning (SFT) of models to generate Triton GPU kernels.

## Dataset Description

The dataset consists of two types of examples:
- **Synthetic queries**: {synthetic_count} examples ({synthetic_count/total_examples*100:.1f}%) - Generated queries asking for Triton kernels for various operations
- **Convert queries**: {convert_count} examples ({convert_count/total_examples*100:.1f}%) - PyTorch code conversion requests to Triton kernels

## Documentation Usage

- **With Triton docs**: {with_docs_count} examples ({with_docs_count/total_examples*100:.1f}%)
- **Without Triton docs**: {without_docs_count} examples ({without_docs_count/total_examples*100:.1f}%)

## Dataset Structure

Each example contains:
- `prompt`: The instruction/query asking for a Triton kernel
- `completion`: The corresponding Triton kernel implementation
- `id`: Unique identifier for the example
- `operation`: The operation type (e.g., "matmul", "softmax", etc.)
- `type`: The query type ("synthetic" or "convert")
- `has_triton_docs`: Boolean indicating if Triton documentation was used during generation
- `pytorch_code`: Original PyTorch code (for convert type examples)

## Usage

```python
from datasets import load_dataset

dataset = load_dataset("{repo_id}")

# Filter by type
synthetic_examples = dataset.filter(lambda x: x['type'] == 'synthetic')
convert_examples = dataset.filter(lambda x: x['type'] == 'convert')

# Filter by documentation usage
with_docs = dataset.filter(lambda x: x['has_triton_docs'])
without_docs = dataset.filter(lambda x: not x['has_triton_docs'])
```

## Generation

This dataset was generated using automated prompting techniques with language models to create diverse training examples for Triton kernel generation. The dataset includes examples generated both with and without access to Triton documentation to enable comparative analysis.
"""
    
    # Upload the dataset
    print(f"Uploading dataset to {repo_id}...")
    try:
        dataset.push_to_hub(
            repo_id,
            private=private,
            commit_message="Upload Triton SFT dataset"
        )
        print(f"✓ Successfully uploaded dataset to: https://huggingface.co/datasets/{repo_id}")
        
        # Upload dataset card
        api.upload_file(
            path_or_fileobj=dataset_card.encode(),
            path_in_repo="README.md",
            repo_id=repo_id,
            repo_type="dataset",
            commit_message="Add dataset card"
        )
        print("✓ Dataset card uploaded")
        
        return True
        
    except Exception as e:
        print(f"✗ Failed to upload dataset: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description="Upload SFT dataset to Hugging Face Hub")
    parser.add_argument("--dataset-path", default="data/sft_dataset_6000.jsonl", help="Path to JSONL dataset file")
    parser.add_argument("--repo-name", default="triton-sft-dataset-6k-v2", help="Repository name on HF Hub")
    parser.add_argument("--username", help="HF username (optional, will be inferred)")
    parser.add_argument("--private", action="store_true", help="Make dataset private")
    
    args = parser.parse_args()
    
    # Verify file exists
    if not os.path.exists(args.dataset_path):
        print(f"✗ Dataset file not found: {args.dataset_path}")
        return
    
    success = upload_dataset_to_hf(
        dataset_path=args.dataset_path,
        repo_name=args.repo_name,
        username=args.username,
        private=args.private
    )
    
    if success:
        print("\n🎉 Dataset upload completed successfully!")
        print("The dataset includes examples both with and without Triton docs for comparative analysis.")
    else:
        print("\n❌ Dataset upload failed. Please check the errors above.")

if __name__ == "__main__":
    main()
